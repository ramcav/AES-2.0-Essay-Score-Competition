{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c10f18",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-21T21:59:16.816149Z",
     "iopub.status.busy": "2024-05-21T21:59:16.815394Z",
     "iopub.status.idle": "2024-05-21T21:59:17.637017Z",
     "shell.execute_reply": "2024-05-21T21:59:17.635835Z"
    },
    "id": "OG-8WumodbKQ",
    "papermill": {
     "duration": 0.835248,
     "end_time": "2024-05-21T21:59:17.639243",
     "exception": false,
     "start_time": "2024-05-21T21:59:16.803995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/random-forest/random_forest_model.pkl\n",
      "/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\n",
      "/kaggle/input/inputs/xgb_aes_model.pkl\n",
      "/kaggle/input/inputs/textstat-0.7.3-py3-none-any.whl\n",
      "/kaggle/input/inputs/vectorizer.pk\n",
      "/kaggle/input/inputs/pyphen-0.15.0-py3-none-any.whl\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/config.json\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/tokenizer.json\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/metadata.json\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/model.weights.h5\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/assets/tokenizer/vocabulary.spm\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60036b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:59:17.659620Z",
     "iopub.status.busy": "2024-05-21T21:59:17.658927Z",
     "iopub.status.idle": "2024-05-21T21:59:17.663262Z",
     "shell.execute_reply": "2024-05-21T21:59:17.662386Z"
    },
    "id": "d35gpEildbKS",
    "papermill": {
     "duration": 0.016884,
     "end_time": "2024-05-21T21:59:17.665607",
     "exception": false,
     "start_time": "2024-05-21T21:59:17.648723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441ae0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:59:17.691161Z",
     "iopub.status.busy": "2024-05-21T21:59:17.690869Z",
     "iopub.status.idle": "2024-05-21T22:00:55.209256Z",
     "shell.execute_reply": "2024-05-21T22:00:55.208252Z"
    },
    "id": "lubd0AZadbKV",
    "papermill": {
     "duration": 97.532902,
     "end_time": "2024-05-21T22:00:55.211674",
     "exception": false,
     "start_time": "2024-05-21T21:59:17.678772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/inputs/pyphen-0.15.0-py3-none-any.whl\r\n",
      "Installing collected packages: pyphen\r\n",
      "Successfully installed pyphen-0.15.0\r\n",
      "Processing /kaggle/input/inputs/textstat-0.7.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyphen in /opt/conda/lib/python3.10/site-packages (from textstat==0.7.3) (0.15.0)\r\n",
      "Installing collected packages: textstat\r\n",
      "Successfully installed textstat-0.7.3\r\n",
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"/kaggle/input/inputs/pyphen-0.15.0-py3-none-any.whl\"\n",
    "!pip install \"/kaggle/input/inputs/textstat-0.7.3-py3-none-any.whl\"\n",
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2b1d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:00:55.234050Z",
     "iopub.status.busy": "2024-05-21T22:00:55.233688Z",
     "iopub.status.idle": "2024-05-21T22:00:56.494669Z",
     "shell.execute_reply": "2024-05-21T22:00:56.493568Z"
    },
    "id": "Wn8NzRhqdbKW",
    "papermill": {
     "duration": 1.275,
     "end_time": "2024-05-21T22:00:56.497175",
     "exception": false,
     "start_time": "2024-05-21T22:00:55.222175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c08cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:00:56.521232Z",
     "iopub.status.busy": "2024-05-21T22:00:56.520866Z",
     "iopub.status.idle": "2024-05-21T22:01:58.681692Z",
     "shell.execute_reply": "2024-05-21T22:01:58.680639Z"
    },
    "id": "l3x3yRQ4dbKY",
    "papermill": {
     "duration": 62.174819,
     "end_time": "2024-05-21T22:01:58.683836",
     "exception": false,
     "start_time": "2024-05-21T22:00:56.509017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "from textstat import textstat\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split  # Import package\n",
    "\n",
    "import pickle # To save models\n",
    "\n",
    "# Ensure you have the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088e353",
   "metadata": {
    "id": "koESPFXVdbKZ",
    "papermill": {
     "duration": 0.010994,
     "end_time": "2024-05-21T22:01:58.705983",
     "exception": false,
     "start_time": "2024-05-21T22:01:58.694989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get existing models (RF + XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f536db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:01:58.729703Z",
     "iopub.status.busy": "2024-05-21T22:01:58.728855Z",
     "iopub.status.idle": "2024-05-21T22:01:58.740052Z",
     "shell.execute_reply": "2024-05-21T22:01:58.739166Z"
    },
    "id": "wc1y_oIIdbKd",
    "papermill": {
     "duration": 0.025246,
     "end_time": "2024-05-21T22:01:58.742075",
     "exception": false,
     "start_time": "2024-05-21T22:01:58.716829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 'kappa',(1.0 - numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32ff28c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:01:58.764962Z",
     "iopub.status.busy": "2024-05-21T22:01:58.764642Z",
     "iopub.status.idle": "2024-05-21T22:02:02.907988Z",
     "shell.execute_reply": "2024-05-21T22:02:02.907118Z"
    },
    "id": "grx5ovktdbKf",
    "papermill": {
     "duration": 4.157506,
     "end_time": "2024-05-21T22:02:02.910431",
     "exception": false,
     "start_time": "2024-05-21T22:01:58.752925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/random-forest/random_forest_model.pkl', 'rb') as rf_file:\n",
    "    rf_model = pickle.load(rf_file)\n",
    "\n",
    "with open('/kaggle/input/inputs/xgb_aes_model.pkl', 'rb') as xgb_file:\n",
    "    xgb_model = pickle.load(xgb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303650a",
   "metadata": {
    "id": "kFLXrcOedbKh",
    "papermill": {
     "duration": 0.010806,
     "end_time": "2024-05-21T22:02:02.932282",
     "exception": false,
     "start_time": "2024-05-21T22:02:02.921476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e1cdd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:02:02.955803Z",
     "iopub.status.busy": "2024-05-21T22:02:02.955067Z",
     "iopub.status.idle": "2024-05-21T22:02:15.766610Z",
     "shell.execute_reply": "2024-05-21T22:02:15.765447Z"
    },
    "id": "I1MbANSIdbKj",
    "papermill": {
     "duration": 12.825463,
     "end_time": "2024-05-21T22:02:15.768596",
     "exception": false,
     "start_time": "2024-05-21T22:02:02.943133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 22:02:04.722150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 22:02:04.722247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 22:02:04.853808: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.15.0\n",
      "Keras: 3.2.1\n",
      "KerasNLP: 0.9.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # \"jax\" or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Class for the evaluation metric which is the Weighted Kappa\n",
    "# This metric is used for ordinal data\n",
    "class WeightedKappa(keras.metrics.Metric):\n",
    "    def __init__(self, num_classes=6, epsilon=1e-6):\n",
    "        super().__init__(name=\"weighted_kappa\")\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        label_vec = keras.ops.arange(num_classes, dtype=keras.backend.floatx())\n",
    "        self.row_label_vec = keras.ops.reshape(label_vec, [1, num_classes])\n",
    "        self.col_label_vec = keras.ops.reshape(label_vec, [num_classes, 1])\n",
    "        col_mat = keras.ops.tile(self.col_label_vec, [1, num_classes])\n",
    "        row_mat = keras.ops.tile(self.row_label_vec, [num_classes, 1])\n",
    "        self.weight_mat = (col_mat - row_mat) ** 2\n",
    "\n",
    "        self.numerator = self.add_weight(name=\"numerator\", initializer=\"zeros\")\n",
    "        self.denominator = self.add_weight(name=\"denominator\", initializer=\"zeros\")\n",
    "        self.o_sum = self.add_weight(name = 'o_sum', initializer = 'zeros')\n",
    "        self.e_sum = self.add_weight(name = 'e_sum', initializer = 'zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, **args):\n",
    "        # revert ordinal regression labels to classification labels\n",
    "        y_true = keras.ops.one_hot(keras.ops.sum(y_true, axis=-1) - 1, 6)\n",
    "        y_pred = keras.ops.one_hot(\n",
    "            keras.ops.sum(keras.ops.cast(y_pred > 0.5, dtype=\"int8\"), axis=-1) - 1, 6\n",
    "        )\n",
    "        # weighted kappa calculation\n",
    "        y_true = keras.ops.cast(y_true, dtype=self.col_label_vec.dtype)\n",
    "        y_pred = keras.ops.cast(y_pred, dtype=self.weight_mat.dtype)\n",
    "        batch_size = keras.ops.shape(y_true)[0]\n",
    "\n",
    "        cat_labels = keras.ops.matmul(y_true, self.col_label_vec)\n",
    "        cat_label_mat = keras.ops.tile(cat_labels, [1, self.num_classes])\n",
    "        row_label_mat = keras.ops.tile(self.row_label_vec, [batch_size, 1])\n",
    "\n",
    "        weight = (cat_label_mat - row_label_mat) ** 2\n",
    "\n",
    "        self.numerator.assign_add(keras.ops.sum(weight * y_pred))\n",
    "        label_dist = keras.ops.sum(y_true, axis=0, keepdims=True)\n",
    "        pred_dist = keras.ops.sum(y_pred, axis=0, keepdims=True)\n",
    "        w_pred_dist = keras.ops.matmul(\n",
    "            self.weight_mat, keras.ops.transpose(pred_dist, [1, 0])\n",
    "        )\n",
    "        self.denominator.assign_add(\n",
    "            keras.ops.sum(keras.ops.matmul(label_dist, w_pred_dist))\n",
    "        )\n",
    "\n",
    "        self.o_sum.assign_add(keras.ops.sum(y_pred))\n",
    "        self.e_sum.assign_add(keras.ops.sum(\n",
    "            keras.ops.matmul(keras.ops.transpose(label_dist, [1, 0]), pred_dist)\n",
    "        ))\n",
    "\n",
    "    def result(self):\n",
    "        return 1.0 - (\n",
    "            keras.ops.divide_no_nan(self.numerator, self.denominator)\n",
    "            * keras.ops.divide_no_nan(self.e_sum, self.o_sum)\n",
    "        )\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.numerator.assign(0)\n",
    "        self.denominator.assign(0)\n",
    "        self.o_sum.assign(0)\n",
    "        self.e_sum.assign(0)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"KerasNLP:\", keras_nlp.__version__)\n",
    "\n",
    "class CFG:\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"deberta_v3_extra_small_en\" # Name of pretrained models\n",
    "    sequence_length = 512  # Input sequence length\n",
    "    epochs = 5 # Training epochs\n",
    "    batch_size = 32  # Batch size\n",
    "    scheduler = 'cosine'  # Learning rate scheduler\n",
    "\n",
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fbefd40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:02:15.792842Z",
     "iopub.status.busy": "2024-05-21T22:02:15.792272Z",
     "iopub.status.idle": "2024-05-21T22:02:17.944064Z",
     "shell.execute_reply": "2024-05-21T22:02:17.943034Z"
    },
    "id": "1oSnFQ_MdbKn",
    "papermill": {
     "duration": 2.166631,
     "end_time": "2024-05-21T22:02:17.946495",
     "exception": false,
     "start_time": "2024-05-21T22:02:15.779864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'tokenizer.json' from model 'keras/deberta_v3/keras/deberta_v3_extra_small_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/deberta_v3/keras/deberta_v3_extra_small_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/deberta_v3/keras/deberta_v3_extra_small_en/2' to your Kaggle notebook...\n"
     ]
    }
   ],
   "source": [
    "# LOAD NEURAL NETWORK\n",
    "\n",
    "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
    "    preset=CFG.preset, # Name of the model\n",
    "    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter\n",
    ")\n",
    "\n",
    "def keras_pipeline():\n",
    "    print(\"----------Reading DF----------\")\n",
    "    df = pd.read_csv(f'{BASE_PATH}/train.csv')  # Read CSV file into a DataFrame\n",
    "\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"score\"])\n",
    "\n",
    "    print(\"----------Transforming Labels----------\")\n",
    "    train_df[\"label\"] = to_ordinal(train_df.score.values).tolist()\n",
    "    valid_df[\"label\"] = to_ordinal(valid_df.score.values).tolist()\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Preprocessing Data and Building Train and Valid Sets----------\")\n",
    "    # Train Data\n",
    "    train_texts = train_df.full_text.tolist()  # Extract training texts\n",
    "    train_labels = np.array(train_df.label.tolist())  # Extract training labels\n",
    "\n",
    "    # Build training dataset\n",
    "    train_ds = build_dataset(\n",
    "        train_texts, train_labels, batch_size=CFG.batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Valid Data\n",
    "    valid_texts = valid_df.full_text.tolist()  # Extract validation texts\n",
    "    valid_labels = np.array(valid_df.label.tolist())  # Extract validation labels\n",
    "\n",
    "    # Build validation dataset\n",
    "    valid_ds = build_dataset(\n",
    "        valid_texts, valid_labels, batch_size=CFG.batch_size, shuffle=False\n",
    "    )\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Set Learning Rate callback to adjust the LR as the model learns----------\")\n",
    "    lr_cb = get_lr_callback(CFG.batch_size, plot=False)\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Save a checkpoint of the best model performance during training----------\")\n",
    "    ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.weights.h5\",\n",
    "    monitor=\"val_weighted_kappa\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"max\",\n",
    "    )\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Create the model----------\")\n",
    "    # Create a DebertaV3Classifier model\n",
    "    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n",
    "        CFG.preset, preprocessor=None, num_classes=6\n",
    "    )\n",
    "    inputs = classifier.input\n",
    "    logits = classifier(inputs)\n",
    "\n",
    "    # Compute final output\n",
    "    outputs = keras.layers.Activation(\"sigmoid\")(logits)\n",
    "\n",
    "    # Build Model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with optimizer, loss, and metrics\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(5e-6),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            WeightedKappa()\n",
    "        ],\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Train the model----------\")\n",
    "    # The model is being trained during 5 epochs (amount of passes through the data)\n",
    "    # The epoch is set to 5, having a greater than could increase predictive power\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.epochs,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[lr_cb, ckpt_cb]\n",
    "    )\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Find epoch with best predictive power----------\")\n",
    "    # Find the epoch with the best validation accuracy\n",
    "    best_epoch = np.argmax(model.history.history['val_weighted_kappa'])\n",
    "    best_score = model.history.history['val_weighted_kappa'][best_epoch]\n",
    "    best_loss = model.history.history['val_loss'][best_epoch]\n",
    "\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Generate predictions----------\")\n",
    "\n",
    "     # Load best checkpoint\n",
    "    model.load_weights(\"best_model.weights.h5\")\n",
    "\n",
    "    # Do inference\n",
    "    train_preds = model.predict(train_ds, verbose=1)\n",
    "\n",
    "    # Convert probabilities to class labels\n",
    "    train_preds = np.sum((train_preds>0.5).astype(int), axis=-1).clip(1, 6)\n",
    "\n",
    "    print(\"----------Done----------\")\n",
    "    print(\"----------Save model and predictions----------\")\n",
    "\n",
    "    return train_preds, model, train_df, valid_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to build the dataset ( apply preprocessing, etc)\n",
    "def build_dataset(texts, labels=None, batch_size=32,\n",
    "                  cache=True, drop_remainder=True,\n",
    "                  shuffle=1024):\n",
    "    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n",
    "    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n",
    "    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n",
    "    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n",
    "    opt = tf.data.Options()  # Create dataset options\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n",
    "        opt.experimental_deterministic = False\n",
    "    ds = ds.with_options(opt)  # Set dataset options\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n",
    "    ds = ds.prefetch(AUTO)  # Prefetch next batch\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Helper func to preprocess data\n",
    "def preprocess_fn(text, label=None):\n",
    "    text = preprocessor(text)  # Preprocess text\n",
    "    return (text, label) if label is not None else text  # Return processed text and label if available\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper functional to convert the label into ordinal\n",
    "def to_ordinal(y, num_classes=None, dtype=\"float32\"):\n",
    "    \"\"\"Converts a class vector (integers) to an ordinal regression matrix.\n",
    "\n",
    "    This utility encodes class vector to ordinal regression/classification\n",
    "    matrix where each sample is indicated by a row and rank of that sample is\n",
    "    indicated by number of ones in that row.\n",
    "\n",
    "    Args:\n",
    "        y: Array-like with class values to be converted into a matrix\n",
    "            (integers from 0 to `num_classes - 1`).\n",
    "        num_classes: Total number of classes. If `None`, this would be inferred\n",
    "            as `max(y) + 1`.\n",
    "        dtype: The data type expected by the input. Default: `'float32'`.\n",
    "\n",
    "    Returns:\n",
    "        An ordinal regression matrix representation of the input as a NumPy\n",
    "        array. The class axis is placed last.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype=\"int\")\n",
    "    input_shape = y.shape\n",
    "\n",
    "    # Shrink the last dimension if the shape is (..., 1).\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "\n",
    "    y = y.reshape(-1)\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    range_values = np.arange(num_classes - 1)\n",
    "    range_values = np.tile(np.expand_dims(range_values, 0), [n, 1])\n",
    "    ordinal = np.zeros((n, num_classes - 1), dtype=dtype)\n",
    "    ordinal[range_values < np.expand_dims(y, -1)] = 1\n",
    "    output_shape = input_shape + (num_classes - 1,)\n",
    "    ordinal = np.reshape(ordinal, output_shape)\n",
    "    return ordinal\n",
    "\n",
    "import math\n",
    "\n",
    "# Update the learning rate\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 0.6e-5, 0.3e-5 * batch_size, 0.3e-5\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.75\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcfba498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:02:17.971261Z",
     "iopub.status.busy": "2024-05-21T22:02:17.970638Z",
     "iopub.status.idle": "2024-05-21T22:02:18.799735Z",
     "shell.execute_reply": "2024-05-21T22:02:18.798923Z"
    },
    "id": "Xy6OlYKAdbKr",
    "papermill": {
     "duration": 0.844121,
     "end_time": "2024-05-21T22:02:18.802128",
     "exception": false,
     "start_time": "2024-05-21T22:02:17.958007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{BASE_PATH}/train.csv')  # Read CSV file into a DataFrame\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec12b85",
   "metadata": {
    "id": "u573BhXFdbKs",
    "papermill": {
     "duration": 0.010848,
     "end_time": "2024-05-21T22:02:18.824738",
     "exception": false,
     "start_time": "2024-05-21T22:02:18.813890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f571a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:02:18.848734Z",
     "iopub.status.busy": "2024-05-21T22:02:18.848399Z",
     "iopub.status.idle": "2024-05-21T22:02:18.873990Z",
     "shell.execute_reply": "2024-05-21T22:02:18.873056Z"
    },
    "id": "Pz2uRIn9dbKs",
    "papermill": {
     "duration": 0.040161,
     "end_time": "2024-05-21T22:02:18.875983",
     "exception": false,
     "start_time": "2024-05-21T22:02:18.835822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def extra_feature_pipeline(df):\n",
    "\n",
    "    print(\"---------CALCULATING ESSAY LENGTHS---------\")\n",
    "    train_df = calculate_essay_lengths(df, \"full_text\")\n",
    "    print(\"---------FINISHED CALCULATING ESSAY LENGTHS---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING SENTIMENTS---------\")\n",
    "    train_df = analyze_sentiment(df, \"full_text\")\n",
    "    print(\"---------FINISHED ANALYZING SENTIMENTS---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING READABILITY---------\")\n",
    "    readability_train_df = analyze_readability(df, \"full_text\")\n",
    "    train_df[\"fkg_score\"] = readability_train_df[\"Flesch-Kincaid\"]\n",
    "    train_df[\"gf_score\"] = readability_train_df[\"Gunning Fog\"]\n",
    "    print(\"---------FINISHED ANALYZING READABILITY---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\")\n",
    "    train_df = lexical_diversity_and_mistakes(df, \"full_text\")\n",
    "    print(\"---------FINISHED ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\\n\")\n",
    "\n",
    "    print(\"---------GETTING DIFFICULT WORD COUNT---------\")\n",
    "    train_df = get_difficult_word_count(df, \"full_text\")\n",
    "    print(\"---------FINISHED GETTING DIFFICULT WORD COUNT---------\\n\")\n",
    "\n",
    "    print(\"---------STARTING PREPROCESSING---------\")\n",
    "    train_df[\"full_text\"] = train_df[\"full_text\"].apply(preprocess_text)\n",
    "    print(\"---------FINISHED PREPROCESSING---------\\n\")\n",
    "\n",
    "    print(\"---------STARTING VECTORIZATION---------\")\n",
    "    with open('/kaggle/input/inputs/vectorizer.pk', 'rb') as file:\n",
    "        vectorizer = pickle.load(file)\n",
    "    train_df_vectorized = add_tfidf_features(df, \"full_text\", vectorizer)\n",
    "    print(\"---------FINISHED VECTORIZATION---------\\n\")\n",
    "\n",
    "    return train_df, train_df_vectorized\n",
    "\n",
    "def add_tfidf_features(train_df,text_column, vectorizer):\n",
    "    train_tfidf = vectorizer.transform(train_df[text_column])\n",
    "\n",
    "    train_dense = train_tfidf.toarray()\n",
    "\n",
    "    train_tfidf_df = pd.DataFrame(train_dense, columns=[f'tfid_{i}' for i in range(train_dense.shape[1])])\n",
    "\n",
    "    train_tfidf_df['essay_id'] = train_df['essay_id'].values\n",
    "\n",
    "    train_df = train_df.merge(train_tfidf_df, on='essay_id', how='left')\n",
    "\n",
    "    return train_df\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "def calculate_essay_lengths(df, text_column):\n",
    "    df['char_essay_length'] = df[text_column].apply(len)\n",
    "    df['words_essay_length'] = df[text_column].apply(lambda x: len(x.split()))\n",
    "    df['sentence_essay_length'] = df[text_column].apply(lambda x: len(x.split('.')))\n",
    "    return df\n",
    "\n",
    "def analyze_sentiment(df, text_column):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Analyze the sentiment of each essay\n",
    "    sentiment_scores = df[text_column].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Add sentiment scores to the dataframe\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "\n",
    "    # Overall sentiment analysis\n",
    "    positive_count = sum(sentiment_scores > 0)\n",
    "    negative_count = sum(sentiment_scores < 0)\n",
    "    neutral_count = len(sentiment_scores) - positive_count - negative_count\n",
    "\n",
    "    # Data for visualization\n",
    "    categories = ['Positive', 'Negative', 'Neutral']\n",
    "    counts = [positive_count, negative_count, neutral_count]\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def analyze_readability(df, text_column):\n",
    "    # Analyzing readability scores for each essay\n",
    "    readability_scores = []\n",
    "    for essay in df[text_column]:\n",
    "        # Compute Flesch-Kincaid Grade Level\n",
    "        fkg_score = textstat.flesch_kincaid_grade(essay)\n",
    "\n",
    "        # Compute Gunning Fog Index\n",
    "        gunning_fog_score = textstat.gunning_fog(essay)\n",
    "\n",
    "        # Add scores to list\n",
    "        readability_scores.append({'Flesch-Kincaid': fkg_score, 'Gunning Fog': gunning_fog_score})\n",
    "\n",
    "    # Creating a dataframe to store the scores\n",
    "    readability_df = pd.DataFrame(readability_scores)\n",
    "\n",
    "    return readability_df\n",
    "\n",
    "# Inspo from https://www.kaggle.com/code/kuangank/ase-fighting\n",
    "def lexical_diversity_and_mistakes(df, text_column):\n",
    "    spell_checker = SpellChecker()\n",
    "\n",
    "    lexical_diversities = []\n",
    "    spelling_mistake_counts = []\n",
    "    spelling_mistake_ratios = []\n",
    "\n",
    "    for text in df[text_column]:\n",
    "        tokens = word_tokenize(text)\n",
    "        unique_tokens = set(tokens)\n",
    "\n",
    "        # Calculate lexical diversity\n",
    "        if len(tokens) == 0:\n",
    "            lexical_diversity = 0\n",
    "        else:\n",
    "            lexical_diversity = len(unique_tokens) / len(tokens)\n",
    "\n",
    "        # Calculate spelling mistakes\n",
    "        spelling_mistake_count = len(spell_checker.unknown(token for token in tokens if token.isalpha()))\n",
    "\n",
    "        # Calculate spelling mistake ratio\n",
    "        if len(tokens) == 0:\n",
    "            spelling_mistake_ratio = 0\n",
    "        else:\n",
    "            spelling_mistake_ratio = spelling_mistake_count / len(tokens)\n",
    "\n",
    "        lexical_diversities.append(lexical_diversity)\n",
    "        spelling_mistake_counts.append(spelling_mistake_count)\n",
    "        spelling_mistake_ratios.append(spelling_mistake_ratio)\n",
    "\n",
    "    df['lexical_diversity'] = lexical_diversities\n",
    "    df['spelling_mistake_count'] = spelling_mistake_counts\n",
    "    df['spelling_mistake_ratio'] = spelling_mistake_ratios\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_difficult_word_count(df, text_column):\n",
    "    difficult_word_counts = []\n",
    "    difficult_word_ratios = []\n",
    "\n",
    "    for text in df[text_column]:\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Calculate difficult words\n",
    "        difficult_word_count = textstat.difficult_words(text)\n",
    "\n",
    "        # Calculate difficult word ratio\n",
    "        if len(tokens) == 0:\n",
    "            difficult_word_ratio = 0\n",
    "        else:\n",
    "            difficult_word_ratio = difficult_word_count / len(tokens)\n",
    "\n",
    "        difficult_word_counts.append(difficult_word_count)\n",
    "        difficult_word_ratios.append(difficult_word_ratio)\n",
    "\n",
    "    df['difficult_words'] = difficult_word_counts\n",
    "    df['difficult_word_ratio'] = difficult_word_ratios\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8830823f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:02:18.901115Z",
     "iopub.status.busy": "2024-05-21T22:02:18.900553Z",
     "iopub.status.idle": "2024-05-21T22:07:53.965916Z",
     "shell.execute_reply": "2024-05-21T22:07:53.964948Z"
    },
    "id": "s6VvZo1idbKw",
    "papermill": {
     "duration": 335.091667,
     "end_time": "2024-05-21T22:07:53.980067",
     "exception": false,
     "start_time": "2024-05-21T22:02:18.888400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------CALCULATING ESSAY LENGTHS---------\n",
      "---------FINISHED CALCULATING ESSAY LENGTHS---------\n",
      "\n",
      "---------ANALYZING SENTIMENTS---------\n",
      "---------FINISHED ANALYZING SENTIMENTS---------\n",
      "\n",
      "---------ANALYZING READABILITY---------\n",
      "---------FINISHED ANALYZING READABILITY---------\n",
      "\n",
      "---------ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "---------FINISHED ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "\n",
      "---------GETTING DIFFICULT WORD COUNT---------\n",
      "---------FINISHED GETTING DIFFICULT WORD COUNT---------\n",
      "\n",
      "---------STARTING PREPROCESSING---------\n",
      "---------FINISHED PREPROCESSING---------\n",
      "\n",
      "---------STARTING VECTORIZATION---------\n",
      "---------FINISHED VECTORIZATION---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_extra, train_df_vect = extra_feature_pipeline(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24323cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:07:54.005980Z",
     "iopub.status.busy": "2024-05-21T22:07:54.005638Z",
     "iopub.status.idle": "2024-05-21T22:09:18.028267Z",
     "shell.execute_reply": "2024-05-21T22:09:18.027302Z"
    },
    "id": "mcmNhlIQdbKw",
    "papermill": {
     "duration": 84.038607,
     "end_time": "2024-05-21T22:09:18.030497",
     "exception": false,
     "start_time": "2024-05-21T22:07:53.991890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------CALCULATING ESSAY LENGTHS---------\n",
      "---------FINISHED CALCULATING ESSAY LENGTHS---------\n",
      "\n",
      "---------ANALYZING SENTIMENTS---------\n",
      "---------FINISHED ANALYZING SENTIMENTS---------\n",
      "\n",
      "---------ANALYZING READABILITY---------\n",
      "---------FINISHED ANALYZING READABILITY---------\n",
      "\n",
      "---------ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "---------FINISHED ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "\n",
      "---------GETTING DIFFICULT WORD COUNT---------\n",
      "---------FINISHED GETTING DIFFICULT WORD COUNT---------\n",
      "\n",
      "---------STARTING PREPROCESSING---------\n",
      "---------FINISHED PREPROCESSING---------\n",
      "\n",
      "---------STARTING VECTORIZATION---------\n",
      "---------FINISHED VECTORIZATION---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_df_extra, valid_df_vect = extra_feature_pipeline(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f871cf4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.057493Z",
     "iopub.status.busy": "2024-05-21T22:09:18.057108Z",
     "iopub.status.idle": "2024-05-21T22:09:18.061265Z",
     "shell.execute_reply": "2024-05-21T22:09:18.060381Z"
    },
    "id": "CIIONW__dbKx",
    "papermill": {
     "duration": 0.020032,
     "end_time": "2024-05-21T22:09:18.063290",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.043258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Valid Data\n",
    "# valid_texts = valid_df.full_text.tolist()  # Extract validation texts\n",
    "# valid_labels = np.array(valid_df.label.tolist())  # Extract validation labels\n",
    "\n",
    "# # Build validation dataset\n",
    "# valid_ds = build_dataset(\n",
    "#         valid_texts, valid_labels, batch_size=CFG.batch_size, shuffle=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92fcb0aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.089883Z",
     "iopub.status.busy": "2024-05-21T22:09:18.089209Z",
     "iopub.status.idle": "2024-05-21T22:09:18.099915Z",
     "shell.execute_reply": "2024-05-21T22:09:18.099156Z"
    },
    "papermill": {
     "duration": 0.0262,
     "end_time": "2024-05-21T22:09:18.101899",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.075699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_df_e = valid_df_extra.drop([\"essay_id\",\"full_text\"], axis = 1)\n",
    "valid_df_v = valid_df_vect.drop([\"essay_id\",\"full_text\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "536255d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.127831Z",
     "iopub.status.busy": "2024-05-21T22:09:18.127524Z",
     "iopub.status.idle": "2024-05-21T22:09:18.133291Z",
     "shell.execute_reply": "2024-05-21T22:09:18.132472Z"
    },
    "papermill": {
     "duration": 0.020853,
     "end_time": "2024-05-21T22:09:18.135149",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.114296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to convert TensorFlow dataset to NumPy array\n",
    "def tf_dataset_to_numpy(tf_dataset):\n",
    "    # Accumulate batches into a list\n",
    "    batches = [batch for batch in tf_dataset]\n",
    "    # If the dataset has labels, unpack the tuples\n",
    "    if isinstance(batches[0], tuple):\n",
    "        # Assume (inputs, labels)\n",
    "        inputs, labels = zip(*batches)\n",
    "        inputs_np = np.concatenate(inputs, axis=0)\n",
    "        labels_np = np.concatenate(labels, axis=0)\n",
    "        return inputs_np, labels_np\n",
    "    else:\n",
    "        inputs_np = np.concatenate(batches, axis=0)\n",
    "        return inputs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f7aab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.160723Z",
     "iopub.status.busy": "2024-05-21T22:09:18.160454Z",
     "iopub.status.idle": "2024-05-21T22:09:18.605070Z",
     "shell.execute_reply": "2024-05-21T22:09:18.603498Z"
    },
    "id": "M3APWOnfdbKy",
    "papermill": {
     "duration": 0.46029,
     "end_time": "2024-05-21T22:09:18.607483",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.147193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Predictions:\n",
      "   rf_predictions\n",
      "0               3\n",
      "1               3\n",
      "2               4\n",
      "3               6\n",
      "4               3\n",
      "XGB Predictions:\n",
      "   xgb_predictions\n",
      "0                2\n",
      "1                2\n",
      "2                3\n",
      "3                5\n",
      "4                2\n"
     ]
    }
   ],
   "source": [
    "# Handle NaNs by dropping rows with any NaNs in valid_df_e and valid_df_v\n",
    "valid_df_e = valid_df_e.dropna().reset_index(drop=True)\n",
    "valid_df_v = valid_df_v.dropna().reset_index(drop=True)\n",
    "\n",
    "y_test = valid_df_e['score']\n",
    "\n",
    "valid_df_e = valid_df_e.drop([\"score\"], axis = 1)\n",
    "valid_df_v = valid_df_v.drop([\"score\"], axis = 1)\n",
    "\n",
    "# Generate predictions from the base models\n",
    "# nn_predictions = model.predict(valid_ds, verbose=1)\n",
    "rf_predictions = rf_model.predict(valid_df_e)\n",
    "xgb_predictions = xgb_model.predict(valid_df_v)\n",
    "\n",
    "# Ensure the predictions are in the correct shape\n",
    "# nn_predictions = nn_predictions.reshape(-1, 1)\n",
    "rf_predictions = rf_predictions.reshape(-1, 1)\n",
    "xgb_predictions = xgb_predictions.reshape(-1, 1)\n",
    "\n",
    "# Output predictions for manual joining\n",
    "# nn_predictions_df = pd.DataFrame(nn_predictions, columns=['nn_predictions'])\n",
    "rf_predictions_df = pd.DataFrame(rf_predictions, columns=['rf_predictions'])\n",
    "xgb_predictions_df = pd.DataFrame(xgb_predictions, columns=['xgb_predictions'])\n",
    "\n",
    "# Print the predictions DataFrames\n",
    "# print(\"NN Predictions:\")\n",
    "# print(nn_predictions_df.head())\n",
    "\n",
    "print(\"RF Predictions:\")\n",
    "print(rf_predictions_df.head())\n",
    "\n",
    "print(\"XGB Predictions:\")\n",
    "print(xgb_predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe443ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.640242Z",
     "iopub.status.busy": "2024-05-21T22:09:18.639579Z",
     "iopub.status.idle": "2024-05-21T22:09:18.645936Z",
     "shell.execute_reply": "2024-05-21T22:09:18.644865Z"
    },
    "papermill": {
     "duration": 0.025411,
     "end_time": "2024-05-21T22:09:18.648191",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.622780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.concat([rf_predictions_df, xgb_predictions_df, valid_df_e], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4c6f97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.680242Z",
     "iopub.status.busy": "2024-05-21T22:09:18.679303Z",
     "iopub.status.idle": "2024-05-21T22:09:18.684618Z",
     "shell.execute_reply": "2024-05-21T22:09:18.683515Z"
    },
    "papermill": {
     "duration": 0.023978,
     "end_time": "2024-05-21T22:09:18.686931",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.662953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a83029ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.716073Z",
     "iopub.status.busy": "2024-05-21T22:09:18.715699Z",
     "iopub.status.idle": "2024-05-21T22:09:18.746910Z",
     "shell.execute_reply": "2024-05-21T22:09:18.745958Z"
    },
    "papermill": {
     "duration": 0.048263,
     "end_time": "2024-05-21T22:09:18.749153",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.700890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(merged_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f53d32b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.777521Z",
     "iopub.status.busy": "2024-05-21T22:09:18.776667Z",
     "iopub.status.idle": "2024-05-21T22:09:18.781665Z",
     "shell.execute_reply": "2024-05-21T22:09:18.780717Z"
    },
    "papermill": {
     "duration": 0.021329,
     "end_time": "2024-05-21T22:09:18.783601",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.762272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming nn_predictions_df is a DataFrame containing the probabilities\n",
    "# Ensure nn_predictions_df is a DataFrame\n",
    "# nn_predictions_df = pd.DataFrame(nn_predictions_df)\n",
    "\n",
    "# # Apply the linear transformation to map probabilities from [0, 1] to [1, 6]\n",
    "# nn_mapped_preds = 1 + nn_predictions_df * 5\n",
    "\n",
    "# # If you need to convert this to integers, you can round or use another method\n",
    "# nn_mapped_preds = nn_mapped_preds.round().astype(int)\n",
    "\n",
    "# # Convert the result to a DataFrame if needed\n",
    "# nn_mapped_preds_df = pd.DataFrame(nn_mapped_preds)\n",
    "\n",
    "# # Print the first few rows to verify\n",
    "# print(nn_mapped_preds_df.head())\n",
    "\n",
    "# # Count the number of rows where the mapped value is 1\n",
    "# count_ones = nn_mapped_preds_df[nn_mapped_preds_df == 1].count().sum()\n",
    "# print(\"Count of rows with value 1:\", count_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f0f1c08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:18.810945Z",
     "iopub.status.busy": "2024-05-21T22:09:18.810591Z",
     "iopub.status.idle": "2024-05-21T22:09:19.209942Z",
     "shell.execute_reply": "2024-05-21T22:09:19.208855Z"
    },
    "papermill": {
     "duration": 0.416044,
     "end_time": "2024-05-21T22:09:19.212550",
     "exception": false,
     "start_time": "2024-05-21T22:09:18.796506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n",
    "submission = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7ab0d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:19.239960Z",
     "iopub.status.busy": "2024-05-21T22:09:19.239644Z",
     "iopub.status.idle": "2024-05-21T22:09:20.858976Z",
     "shell.execute_reply": "2024-05-21T22:09:20.857714Z"
    },
    "papermill": {
     "duration": 1.635336,
     "end_time": "2024-05-21T22:09:20.861022",
     "exception": false,
     "start_time": "2024-05-21T22:09:19.225686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------CALCULATING ESSAY LENGTHS---------\n",
      "---------FINISHED CALCULATING ESSAY LENGTHS---------\n",
      "\n",
      "---------ANALYZING SENTIMENTS---------\n",
      "---------FINISHED ANALYZING SENTIMENTS---------\n",
      "\n",
      "---------ANALYZING READABILITY---------\n",
      "---------FINISHED ANALYZING READABILITY---------\n",
      "\n",
      "---------ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "---------FINISHED ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "\n",
      "---------GETTING DIFFICULT WORD COUNT---------\n",
      "---------FINISHED GETTING DIFFICULT WORD COUNT---------\n",
      "\n",
      "---------STARTING PREPROCESSING---------\n",
      "---------FINISHED PREPROCESSING---------\n",
      "\n",
      "---------STARTING VECTORIZATION---------\n",
      "---------FINISHED VECTORIZATION---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_rf, test_df_vectorized = extra_feature_pipeline(test)\n",
    "\n",
    "test_df_rf = test_df_rf.dropna()\n",
    "test_df_vectorized = test_df_vectorized.dropna()\n",
    "\n",
    "test_df_rf = test_df_rf.drop([\"essay_id\",\"full_text\"],axis=1)\n",
    "test_df_vectorized = test_df_vectorized.drop([\"essay_id\",\"full_text\"],axis=1)\n",
    "\n",
    "\n",
    "# Generate predictions from the base models\n",
    "# nn_predictions = model.predict(valid_ds, verbose=1)\n",
    "rf_predictions_final = rf_model.predict(test_df_rf)\n",
    "xgb_predictions = xgb_model.predict(test_df_vectorized)\n",
    "\n",
    "# Ensure the predictions are in the correct shape\n",
    "# nn_predictions = nn_predictions.reshape(-1, 1)\n",
    "rf_predictions_final = rf_predictions_final.reshape(-1, 1)\n",
    "xgb_predictions = xgb_predictions.reshape(-1, 1)\n",
    "\n",
    "# Output predictions for manual joining\n",
    "# nn_predictions_df = pd.DataFrame(nn_predictions, columns=['nn_predictions'])\n",
    "rf_predictions_df_final = pd.DataFrame(rf_predictions_final, columns=['rf_predictions'])\n",
    "xgb_predictions_df_final = pd.DataFrame(xgb_predictions, columns=['xgb_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5799386f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:20.889548Z",
     "iopub.status.busy": "2024-05-21T22:09:20.888847Z",
     "iopub.status.idle": "2024-05-21T22:09:20.907910Z",
     "shell.execute_reply": "2024-05-21T22:09:20.906980Z"
    },
    "papermill": {
     "duration": 0.035951,
     "end_time": "2024-05-21T22:09:20.910173",
     "exception": false,
     "start_time": "2024-05-21T22:09:20.874222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf_predictions</th>\n",
       "      <th>xgb_predictions</th>\n",
       "      <th>char_essay_length</th>\n",
       "      <th>words_essay_length</th>\n",
       "      <th>sentence_essay_length</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>fkg_score</th>\n",
       "      <th>gf_score</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>spelling_mistake_count</th>\n",
       "      <th>spelling_mistake_ratio</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>difficult_word_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2677</td>\n",
       "      <td>498</td>\n",
       "      <td>14</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>14.7</td>\n",
       "      <td>17.33</td>\n",
       "      <td>0.455046</td>\n",
       "      <td>16</td>\n",
       "      <td>0.029358</td>\n",
       "      <td>60</td>\n",
       "      <td>0.110092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1669</td>\n",
       "      <td>332</td>\n",
       "      <td>20</td>\n",
       "      <td>0.7705</td>\n",
       "      <td>5.4</td>\n",
       "      <td>7.48</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>7</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>24</td>\n",
       "      <td>0.064690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3077</td>\n",
       "      <td>550</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.9731</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.49</td>\n",
       "      <td>0.401653</td>\n",
       "      <td>7</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>67</td>\n",
       "      <td>0.110744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rf_predictions  xgb_predictions  char_essay_length  words_essay_length  \\\n",
       "0               3                2               2677                 498   \n",
       "1               3                2               1669                 332   \n",
       "2               4                3               3077                 550   \n",
       "\n",
       "   sentence_essay_length  sentiment_score  fkg_score  gf_score  \\\n",
       "0                     14           0.9937       14.7     17.33   \n",
       "1                     20           0.7705        5.4      7.48   \n",
       "2                     25          -0.9731        9.9     11.49   \n",
       "\n",
       "   lexical_diversity  spelling_mistake_count  spelling_mistake_ratio  \\\n",
       "0           0.455046                      16                0.029358   \n",
       "1           0.452830                       7                0.018868   \n",
       "2           0.401653                       7                0.011570   \n",
       "\n",
       "   difficult_words  difficult_word_ratio  \n",
       "0               60              0.110092  \n",
       "1               24              0.064690  \n",
       "2               67              0.110744  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds = pd.concat([rf_predictions_df_final, xgb_predictions_df_final, test_df_rf], axis=1)\n",
    "final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b904ccd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:20.939682Z",
     "iopub.status.busy": "2024-05-21T22:09:20.938785Z",
     "iopub.status.idle": "2024-05-21T22:09:20.945265Z",
     "shell.execute_reply": "2024-05-21T22:09:20.944481Z"
    },
    "papermill": {
     "duration": 0.023285,
     "end_time": "2024-05-21T22:09:20.947362",
     "exception": false,
     "start_time": "2024-05-21T22:09:20.924077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classifier.fit(final_preds, y_test)\n",
    "val_pred = classifier.predict(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b03bef9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:20.976686Z",
     "iopub.status.busy": "2024-05-21T22:09:20.975949Z",
     "iopub.status.idle": "2024-05-21T22:09:20.982263Z",
     "shell.execute_reply": "2024-05-21T22:09:20.981354Z"
    },
    "papermill": {
     "duration": 0.023361,
     "end_time": "2024-05-21T22:09:20.984281",
     "exception": false,
     "start_time": "2024-05-21T22:09:20.960920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e7b6c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:21.013603Z",
     "iopub.status.busy": "2024-05-21T22:09:21.012979Z",
     "iopub.status.idle": "2024-05-21T22:09:21.020303Z",
     "shell.execute_reply": "2024-05-21T22:09:21.019413Z"
    },
    "papermill": {
     "duration": 0.024769,
     "end_time": "2024-05-21T22:09:21.022671",
     "exception": false,
     "start_time": "2024-05-21T22:09:20.997902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission['score'] = classifier.predict(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd178ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:09:21.053472Z",
     "iopub.status.busy": "2024-05-21T22:09:21.053102Z",
     "iopub.status.idle": "2024-05-21T22:09:21.060197Z",
     "shell.execute_reply": "2024-05-21T22:09:21.059390Z"
    },
    "papermill": {
     "duration": 0.02465,
     "end_time": "2024-05-21T22:09:21.062314",
     "exception": false,
     "start_time": "2024-05-21T22:09:21.037664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "final-notebook",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 3596984,
     "sourceId": 6258399,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5057716,
     "sourceId": 8479955,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5057990,
     "sourceId": 8480312,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 4684,
     "sourceId": 6063,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 610.508524,
   "end_time": "2024-05-21T22:09:24.416765",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-21T21:59:13.908241",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
