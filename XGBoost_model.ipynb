{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a2596d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-20T18:42:17.294717Z",
     "iopub.status.busy": "2024-05-20T18:42:17.294335Z",
     "iopub.status.idle": "2024-05-20T18:42:18.308407Z",
     "shell.execute_reply": "2024-05-20T18:42:18.307050Z"
    },
    "papermill": {
     "duration": 1.026805,
     "end_time": "2024-05-20T18:42:18.311067",
     "exception": false,
     "start_time": "2024-05-20T18:42:17.284262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n",
      "/kaggle/input/pyphen/pyphen-0.15.0-py3-none-any.whl\n",
      "/kaggle/input/preprocessed-data/preprocessed_train.csv\n",
      "/kaggle/input/preprocessed-data/preprocessed_valid.csv\n",
      "/kaggle/input/textstat/textstat-0.7.3-py3-none-any.whl\n",
      "/kaggle/input/xgb-model/xgb_aes_model.pkl\n",
      "/kaggle/input/xgb-model/vectorizer.pk\n",
      "/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d225f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:42:18.328954Z",
     "iopub.status.busy": "2024-05-20T18:42:18.328461Z",
     "iopub.status.idle": "2024-05-20T18:44:04.836686Z",
     "shell.execute_reply": "2024-05-20T18:44:04.835284Z"
    },
    "papermill": {
     "duration": 106.520761,
     "end_time": "2024-05-20T18:44:04.839810",
     "exception": false,
     "start_time": "2024-05-20T18:42:18.319049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pyphen/pyphen-0.15.0-py3-none-any.whl\r\n",
      "Installing collected packages: pyphen\r\n",
      "Successfully installed pyphen-0.15.0\r\n",
      "Processing /kaggle/input/textstat/textstat-0.7.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyphen in /opt/conda/lib/python3.10/site-packages (from textstat==0.7.3) (0.15.0)\r\n",
      "Installing collected packages: textstat\r\n",
      "Successfully installed textstat-0.7.3\r\n",
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"/kaggle/input/pyphen/pyphen-0.15.0-py3-none-any.whl\"\n",
    "!pip install \"/kaggle/input/textstat/textstat-0.7.3-py3-none-any.whl\"\n",
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151fadb",
   "metadata": {
    "papermill": {
     "duration": 0.008704,
     "end_time": "2024-05-20T18:44:04.857612",
     "exception": false,
     "start_time": "2024-05-20T18:44:04.848908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db718627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:44:04.878108Z",
     "iopub.status.busy": "2024-05-20T18:44:04.877616Z",
     "iopub.status.idle": "2024-05-20T18:44:06.355015Z",
     "shell.execute_reply": "2024-05-20T18:44:06.353812Z"
    },
    "papermill": {
     "duration": 1.491567,
     "end_time": "2024-05-20T18:44:06.358040",
     "exception": false,
     "start_time": "2024-05-20T18:44:04.866473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f9693a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:44:06.379021Z",
     "iopub.status.busy": "2024-05-20T18:44:06.378551Z",
     "iopub.status.idle": "2024-05-20T18:44:06.384564Z",
     "shell.execute_reply": "2024-05-20T18:44:06.383518Z"
    },
    "papermill": {
     "duration": 0.01952,
     "end_time": "2024-05-20T18:44:06.387073",
     "exception": false,
     "start_time": "2024-05-20T18:44:06.367553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00aa9b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:44:06.407846Z",
     "iopub.status.busy": "2024-05-20T18:44:06.407461Z",
     "iopub.status.idle": "2024-05-20T18:45:10.627777Z",
     "shell.execute_reply": "2024-05-20T18:45:10.626614Z"
    },
    "papermill": {
     "duration": 64.233692,
     "end_time": "2024-05-20T18:45:10.630557",
     "exception": false,
     "start_time": "2024-05-20T18:44:06.396865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "from textstat import textstat\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "import pickle # To save models\n",
    "\n",
    "# Ensure you have the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17319d74",
   "metadata": {
    "papermill": {
     "duration": 0.010174,
     "end_time": "2024-05-20T18:45:10.651022",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.640848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8e428b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.673904Z",
     "iopub.status.busy": "2024-05-20T18:45:10.672953Z",
     "iopub.status.idle": "2024-05-20T18:45:10.710777Z",
     "shell.execute_reply": "2024-05-20T18:45:10.709176Z"
    },
    "papermill": {
     "duration": 0.052547,
     "end_time": "2024-05-20T18:45:10.713679",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.661132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "\n",
    "def feature_pipeline():\n",
    "    df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "    \n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"score\"])\n",
    "\n",
    "    print(\"---------CALCULATING ESSAY LENGTHS---------\")\n",
    "    train_df = calculate_essay_lengths(train_df, \"full_text\")\n",
    "    valid_df = calculate_essay_lengths(valid_df, \"full_text\")\n",
    "    print(\"---------FINISHED CALCULATING ESSAY LENGTHS---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING SENTIMENTS---------\")\n",
    "    train_df = analyze_sentiment(train_df, \"full_text\")\n",
    "    valid_df = analyze_sentiment(valid_df, \"full_text\")\n",
    "    print(\"---------FINISHED ANALYZING SENTIMENTS---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING READABILITY---------\")\n",
    "    readability_train_df = analyze_readability(train_df, \"full_text\")\n",
    "    readability_valid_df = analyze_readability(valid_df, \"full_text\")\n",
    "    train_df[\"fkg_score\"] = readability_train_df[\"Flesch-Kincaid\"]\n",
    "    train_df[\"gf_score\"] = readability_train_df[\"Gunning Fog\"]\n",
    "    valid_df[\"fkg_score\"] = readability_valid_df[\"Flesch-Kincaid\"]\n",
    "    valid_df[\"gf_score\"] = readability_valid_df[\"Gunning Fog\"]\n",
    "    print(\"---------FINISHED ANALYZING READABILITY---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\")\n",
    "    train_df = lexical_diversity_and_mistakes(train_df, \"full_text\")\n",
    "    valid_df = lexical_diversity_and_mistakes(valid_df, \"full_text\")\n",
    "    print(\"---------FINISHED ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\\n\")\n",
    "\n",
    "    print(\"---------GETTING DIFFICULT WORD COUNT---------\")\n",
    "    train_df = get_difficult_word_count(train_df, \"full_text\")\n",
    "    valid_df = get_difficult_word_count(valid_df, \"full_text\")\n",
    "    print(\"---------FINISHED GETTING DIFFICULT WORD COUNT---------\\n\")\n",
    "\n",
    "    print(\"---------STARTING PREPROCESSING---------\")\n",
    "    train_df[\"full_text\"] = train_df[\"full_text\"].apply(preprocess_text)\n",
    "    valid_df[\"full_text\"] = valid_df[\"full_text\"].apply(preprocess_text)\n",
    "    print(\"---------FINISHED PREPROCESSING---------\\n\")\n",
    "\n",
    "    print(\"---------STARTING VECTORIZATION OF TEXTS USING TFIDF---------\")\n",
    "    train_df, valid_df, vectorizer = add_tfidf_features(train_df, valid_df, \"full_text\")\n",
    "    print(\"DONE\\n\")\n",
    "\n",
    "    return train_df, valid_df, vectorizer\n",
    "\n",
    "\n",
    "   \n",
    "                                                 \n",
    "# Helper Functions\n",
    "\n",
    "def calculate_essay_lengths(df, text_column):\n",
    "    df['char_essay_length'] = df[text_column].apply(len)\n",
    "    df['words_essay_length'] = df[text_column].apply(lambda x: len(x.split()))\n",
    "    df['sentence_essay_length'] = df[text_column].apply(lambda x: len(x.split('.')))\n",
    "    return df\n",
    "\n",
    "def analyze_sentiment(df, text_column):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Analyze the sentiment of each essay\n",
    "    sentiment_scores = df[text_column].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    \n",
    "    # Add sentiment scores to the dataframe\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    \n",
    "    # Overall sentiment analysis\n",
    "    positive_count = sum(sentiment_scores > 0)\n",
    "    negative_count = sum(sentiment_scores < 0)\n",
    "    neutral_count = len(sentiment_scores) - positive_count - negative_count\n",
    "    \n",
    "    # Data for visualization\n",
    "    categories = ['Positive', 'Negative', 'Neutral']\n",
    "    counts = [positive_count, negative_count, neutral_count]\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def analyze_readability(df, text_column):\n",
    "    # Analyzing readability scores for each essay\n",
    "    readability_scores = []\n",
    "    for essay in df[text_column]:\n",
    "        # Compute Flesch-Kincaid Grade Level\n",
    "        fkg_score = textstat.flesch_kincaid_grade(essay)\n",
    "        \n",
    "        # Compute Gunning Fog Index\n",
    "        gunning_fog_score = textstat.gunning_fog(essay)\n",
    "        \n",
    "        # Add scores to list\n",
    "        readability_scores.append({'Flesch-Kincaid': fkg_score, 'Gunning Fog': gunning_fog_score})\n",
    "    \n",
    "    # Creating a dataframe to store the scores\n",
    "    readability_df = pd.DataFrame(readability_scores)\n",
    "    \n",
    "    return readability_df\n",
    "\n",
    "# Inspo from https://www.kaggle.com/code/kuangank/ase-fighting\n",
    "def lexical_diversity_and_mistakes(df, text_column):\n",
    "    spell_checker = SpellChecker()\n",
    "    \n",
    "    lexical_diversities = []\n",
    "    spelling_mistake_counts = []\n",
    "    spelling_mistake_ratios = []\n",
    "    \n",
    "    for text in df[text_column]:\n",
    "        tokens = word_tokenize(text)\n",
    "        unique_tokens = set(tokens)\n",
    "        \n",
    "        # Calculate lexical diversity\n",
    "        if len(tokens) == 0:\n",
    "            lexical_diversity = 0\n",
    "        else:\n",
    "            lexical_diversity = len(unique_tokens) / len(tokens)\n",
    "        \n",
    "        # Calculate spelling mistakes\n",
    "        spelling_mistake_count = len(spell_checker.unknown(token for token in tokens if token.isalpha()))\n",
    "        \n",
    "        # Calculate spelling mistake ratio\n",
    "        if len(tokens) == 0:\n",
    "            spelling_mistake_ratio = 0\n",
    "        else:\n",
    "            spelling_mistake_ratio = spelling_mistake_count / len(tokens)\n",
    "        \n",
    "        lexical_diversities.append(lexical_diversity)\n",
    "        spelling_mistake_counts.append(spelling_mistake_count)\n",
    "        spelling_mistake_ratios.append(spelling_mistake_ratio)\n",
    "    \n",
    "    df['lexical_diversity'] = lexical_diversities\n",
    "    df['spelling_mistake_count'] = spelling_mistake_counts\n",
    "    df['spelling_mistake_ratio'] = spelling_mistake_ratios\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_difficult_word_count(df, text_column):\n",
    "    difficult_word_counts = []\n",
    "    difficult_word_ratios = []\n",
    "    \n",
    "    for text in df[text_column]:\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Calculate difficult words\n",
    "        difficult_word_count = textstat.difficult_words(text)\n",
    "        \n",
    "        # Calculate difficult word ratio\n",
    "        if len(tokens) == 0:\n",
    "            difficult_word_ratio = 0\n",
    "        else:\n",
    "            difficult_word_ratio = difficult_word_count / len(tokens)\n",
    "        \n",
    "        difficult_word_counts.append(difficult_word_count)\n",
    "        difficult_word_ratios.append(difficult_word_ratio)\n",
    "    \n",
    "    df['difficult_words'] = difficult_word_counts\n",
    "    df['difficult_word_ratio'] = difficult_word_ratios\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Inspo from: https://www.kaggle.com/code/lebinhthanh/baseline-tfidf-lgbm\n",
    "\n",
    "def add_tfidf_features(train_df, valid_df, text_column):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=0.05,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "    )\n",
    "    \n",
    "    train_tfidf = vectorizer.fit_transform(train_df[text_column])\n",
    "    valid_tfidf = vectorizer.transform(valid_df[text_column])\n",
    "    \n",
    "    train_dense = train_tfidf.toarray()\n",
    "    valid_dense = valid_tfidf.toarray()\n",
    "    \n",
    "    train_tfidf_df = pd.DataFrame(train_dense, columns=[f'tfid_{i}' for i in range(train_dense.shape[1])])\n",
    "    valid_tfidf_df = pd.DataFrame(valid_dense, columns=[f'tfid_{i}' for i in range(valid_dense.shape[1])])\n",
    "    \n",
    "    train_tfidf_df['essay_id'] = train_df['essay_id'].values\n",
    "    valid_tfidf_df['essay_id'] = valid_df['essay_id'].values\n",
    "    \n",
    "    train_df = train_df.merge(train_tfidf_df, on='essay_id', how='left')\n",
    "    valid_df = valid_df.merge(valid_tfidf_df, on='essay_id', how='left')\n",
    "    \n",
    "    return train_df, valid_df, vectorizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68ec39",
   "metadata": {
    "papermill": {
     "duration": 0.009657,
     "end_time": "2024-05-20T18:45:10.733310",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.723653",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623b3519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.756713Z",
     "iopub.status.busy": "2024-05-20T18:45:10.756294Z",
     "iopub.status.idle": "2024-05-20T18:45:10.761412Z",
     "shell.execute_reply": "2024-05-20T18:45:10.760090Z"
    },
    "papermill": {
     "duration": 0.019351,
     "end_time": "2024-05-20T18:45:10.763918",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.744567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_feats, valid_feats, vectorizer = feature_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7b663e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.786931Z",
     "iopub.status.busy": "2024-05-20T18:45:10.786230Z",
     "iopub.status.idle": "2024-05-20T18:45:10.790604Z",
     "shell.execute_reply": "2024-05-20T18:45:10.789612Z"
    },
    "papermill": {
     "duration": 0.018595,
     "end_time": "2024-05-20T18:45:10.793040",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.774445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save preprocessed/engineered df\n",
    "\n",
    "\n",
    "# preprocessed_train_df = pre_train.copy()\n",
    "# preprocessed_valid_df = pre_valid.copy()\n",
    "\n",
    "# Save\n",
    "# preprocessed_train_df.to_csv('preprocessed_train.csv',index=False)\n",
    "# preprocessed_valid_df.to_csv('preprocessed_valid.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9891cf",
   "metadata": {
    "papermill": {
     "duration": 0.009471,
     "end_time": "2024-05-20T18:45:10.812341",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.802870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6f00f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.833836Z",
     "iopub.status.busy": "2024-05-20T18:45:10.833407Z",
     "iopub.status.idle": "2024-05-20T18:45:10.839364Z",
     "shell.execute_reply": "2024-05-20T18:45:10.837896Z"
    },
    "papermill": {
     "duration": 0.019698,
     "end_time": "2024-05-20T18:45:10.841909",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.822211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_feats =  pd.read_csv(f'/kaggle/input/preprocessed-data/preprocessed_train.csv')\n",
    "# valid_feats = pd.read_csv(f'/kaggle/input/preprocessed-data/preprocessed_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52093dea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.865208Z",
     "iopub.status.busy": "2024-05-20T18:45:10.863840Z",
     "iopub.status.idle": "2024-05-20T18:45:10.872979Z",
     "shell.execute_reply": "2024-05-20T18:45:10.871847Z"
    },
    "papermill": {
     "duration": 0.023781,
     "end_time": "2024-05-20T18:45:10.875662",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.851881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_feats = train_feats.drop([\"essay_id\", \"full_text\"], axis = 1)\\nfeature_names = train_feats.columns\\n\\nvalid_feats = valid_feats.drop([\"essay_id\", \"full_text\"], axis = 1)\\nv_feature_names = valid_feats.columns\\n\\ntrain_feats[\"score\"] = train_feats[\"score\"] - 1\\nvalid_feats[\"score\"] = valid_feats[\"score\"] - 1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_feats = train_feats.drop([\"essay_id\", \"full_text\"], axis = 1)\n",
    "feature_names = train_feats.columns\n",
    "\n",
    "valid_feats = valid_feats.drop([\"essay_id\", \"full_text\"], axis = 1)\n",
    "v_feature_names = valid_feats.columns\n",
    "\n",
    "train_feats[\"score\"] = train_feats[\"score\"] - 1\n",
    "valid_feats[\"score\"] = valid_feats[\"score\"] - 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8d21ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.898074Z",
     "iopub.status.busy": "2024-05-20T18:45:10.897622Z",
     "iopub.status.idle": "2024-05-20T18:45:10.916123Z",
     "shell.execute_reply": "2024-05-20T18:45:10.914845Z"
    },
    "papermill": {
     "duration": 0.032657,
     "end_time": "2024-05-20T18:45:10.918458",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.885801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspiration From: https://www.kaggle.com/code/mobenmo/get-started-xgboost-eda-0-55-score\n",
    "\n",
    "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 'kappa',(1.0 - numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd25942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.940521Z",
     "iopub.status.busy": "2024-05-20T18:45:10.940104Z",
     "iopub.status.idle": "2024-05-20T18:45:10.948521Z",
     "shell.execute_reply": "2024-05-20T18:45:10.947203Z"
    },
    "papermill": {
     "duration": 0.022576,
     "end_time": "2024-05-20T18:45:10.951038",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.928462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# defining an xgboost regressor\\n# regularisation parameters taken from: https://www.kaggle.com/code/lebinhthanh/baseline-tfidf-lgbm/notebook#Features-engineering\\nxg_reg = xgb.XGBClassifier(\\n            objective=\\'multi:softmax\\',\\n            num_class = 6,\\n            tree_method=\"hist\",\\n            n_estimators=2000,\\n            learning_rate=0.0075,\\n            reg_lambda = 0.1,\\n            reg_alpha = 0.8,\\n            max_leaves = 17,\\n            subsample=0.50,\\n            colsample_bytree=0.50,\\n            max_bin=4096,\\n            n_jobs=2,\\n            feval=quadratic_weighted_kappa,\\n            #eval_metric = quadratic_weighted_kappa,\\n            #eval_metric=\\'auc\\',\\n            early_stopping_rounds=70,\\n        )\\nfeature_names = [col for col in train_feats.columns if col not in [\\'essay_id\\', \\'full_text\\', \\'score\\']]\\nxg_reg.fit(train_feats[feature_names], train_feats[\"score\"], \\n           eval_set=[(train_feats[feature_names], train_feats[\"score\"]), (valid_feats[feature_names], valid_feats[\"score\"])], \\n           verbose=200)\\n           \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# defining an xgboost regressor\n",
    "# regularisation parameters taken from: https://www.kaggle.com/code/lebinhthanh/baseline-tfidf-lgbm/notebook#Features-engineering\n",
    "xg_reg = xgb.XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class = 6,\n",
    "            tree_method=\"hist\",\n",
    "            n_estimators=2000,\n",
    "            learning_rate=0.0075,\n",
    "            reg_lambda = 0.1,\n",
    "            reg_alpha = 0.8,\n",
    "            max_leaves = 17,\n",
    "            subsample=0.50,\n",
    "            colsample_bytree=0.50,\n",
    "            max_bin=4096,\n",
    "            n_jobs=2,\n",
    "            feval=quadratic_weighted_kappa,\n",
    "            #eval_metric = quadratic_weighted_kappa,\n",
    "            #eval_metric='auc',\n",
    "            early_stopping_rounds=70,\n",
    "        )\n",
    "feature_names = [col for col in train_feats.columns if col not in ['essay_id', 'full_text', 'score']]\n",
    "xg_reg.fit(train_feats[feature_names], train_feats[\"score\"], \n",
    "           eval_set=[(train_feats[feature_names], train_feats[\"score\"]), (valid_feats[feature_names], valid_feats[\"score\"])], \n",
    "           verbose=200)\n",
    "           \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e4309b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:10.973371Z",
     "iopub.status.busy": "2024-05-20T18:45:10.972930Z",
     "iopub.status.idle": "2024-05-20T18:45:10.977691Z",
     "shell.execute_reply": "2024-05-20T18:45:10.976323Z"
    },
    "papermill": {
     "duration": 0.019085,
     "end_time": "2024-05-20T18:45:10.980343",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.961258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the vectorizer and model\n",
    "#with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(vectorizer, f)\n",
    "\n",
    "#with open(f\"xgb_aes_model.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(xg_reg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c67e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:11.003729Z",
     "iopub.status.busy": "2024-05-20T18:45:11.003327Z",
     "iopub.status.idle": "2024-05-20T18:45:11.008436Z",
     "shell.execute_reply": "2024-05-20T18:45:11.007355Z"
    },
    "papermill": {
     "duration": 0.020141,
     "end_time": "2024-05-20T18:45:11.010917",
     "exception": false,
     "start_time": "2024-05-20T18:45:10.990776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c743e6",
   "metadata": {
    "papermill": {
     "duration": 0.00995,
     "end_time": "2024-05-20T18:45:11.031140",
     "exception": false,
     "start_time": "2024-05-20T18:45:11.021190",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc823287",
   "metadata": {
    "papermill": {
     "duration": 0.010061,
     "end_time": "2024-05-20T18:45:11.051713",
     "exception": false,
     "start_time": "2024-05-20T18:45:11.041652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54aa531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:45:11.075526Z",
     "iopub.status.busy": "2024-05-20T18:45:11.075135Z",
     "iopub.status.idle": "2024-05-20T18:45:16.923278Z",
     "shell.execute_reply": "2024-05-20T18:45:16.921771Z"
    },
    "papermill": {
     "duration": 5.863636,
     "end_time": "2024-05-20T18:45:16.926070",
     "exception": false,
     "start_time": "2024-05-20T18:45:11.062434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------CALCULATING ESSAY LENGTHS---------\n",
      "---------FINISHED CALCULATING ESSAY LENGTHS---------\n",
      "\n",
      "---------ANALYZING SENTIMENTS---------\n",
      "---------FINISHED ANALYZING SENTIMENTS---------\n",
      "\n",
      "---------ANALYZING READABILITY---------\n",
      "---------FINISHED ANALYZING READABILITY---------\n",
      "\n",
      "---------ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "---------FINISHED ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\n",
      "\n",
      "---------GETTING DIFFICULT WORD COUNT---------\n",
      "---------FINISHED GETTING DIFFICULT WORD COUNT---------\n",
      "\n",
      "---------STARTING PREPROCESSING---------\n",
      "---------FINISHED PREPROCESSING---------\n",
      "\n",
      "---------STARTING VECTORIZATION OF TEXTS USING TFIDF---------\n",
      "DONE\n",
      "\n",
      "  essay_id  score\n",
      "0  000d118      2\n",
      "1  000fe60      2\n",
      "2  001ab80      3\n"
     ]
    }
   ],
   "source": [
    "def xgb_testing_pipeline():\n",
    "    # Load the test data\n",
    "    test_df = pd.read_csv(f\"{BASE_PATH}/test.csv\")\n",
    "\n",
    "    print(\"---------CALCULATING ESSAY LENGTHS---------\")\n",
    "    test_df = calculate_essay_lengths(test_df, \"full_text\")\n",
    "    print(\"---------FINISHED CALCULATING ESSAY LENGTHS---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING SENTIMENTS---------\")\n",
    "    test_df = analyze_sentiment(test_df, \"full_text\")\n",
    "    print(\"---------FINISHED ANALYZING SENTIMENTS---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING READABILITY---------\")\n",
    "    readability_test_df = analyze_readability(test_df, \"full_text\")\n",
    "    test_df[\"fkg_score\"] = readability_test_df[\"Flesch-Kincaid\"]\n",
    "    test_df[\"gf_score\"] = readability_test_df[\"Gunning Fog\"]\n",
    "    print(\"---------FINISHED ANALYZING READABILITY---------\\n\")\n",
    "\n",
    "    print(\"---------ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\")\n",
    "    test_df = lexical_diversity_and_mistakes(test_df, \"full_text\")\n",
    "    print(\"---------FINISHED ANALYZING LEXICAL DIVERSITY AND SPELLING MISTAKES---------\\n\")\n",
    "\n",
    "    print(\"---------GETTING DIFFICULT WORD COUNT---------\")\n",
    "    test_df = get_difficult_word_count(test_df, \"full_text\")\n",
    "    print(\"---------FINISHED GETTING DIFFICULT WORD COUNT---------\\n\")\n",
    "\n",
    "    print(\"---------STARTING PREPROCESSING---------\")\n",
    "    test_df[\"full_text\"] = test_df[\"full_text\"].apply(preprocess_text)\n",
    "    print(\"---------FINISHED PREPROCESSING---------\\n\")\n",
    "\n",
    "    print(\"---------STARTING VECTORIZATION OF TEXTS USING TFIDF---------\")\n",
    "    test_df = add_tfidf_features_to_test(test_df, \"full_text\")\n",
    "    print(\"DONE\\n\")\n",
    "\n",
    "    # Load the trained model\n",
    "    with open(f\"/kaggle/input/xgb-model/xgb_aes_model.pkl\", \"rb\") as f:\n",
    "        xg_reg = pickle.load(f)\n",
    "\n",
    "    # Generating predictions\n",
    "    feature_names = [col for col in test_df.columns if col not in ['essay_id', 'full_text']]  # Exclude non-feature columns\n",
    "    predictions = xg_reg.predict(test_df[feature_names])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def add_tfidf_features_to_test(test_df, text_column):\n",
    "    # Load the fitted TF-IDF vectorizer from your training phase\n",
    "    with open(f\"/kaggle/input/xgb-model/vectorizer.pk\", \"rb\") as f:\n",
    "        tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "    # Transform the test text data using the loaded vectorizer\n",
    "    test_tfidf = tfidf_vectorizer.transform(test_df[text_column])\n",
    "    \n",
    "    # Convert to dense matrix\n",
    "    test_dense = test_tfidf.toarray()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    test_tfidf_df = pd.DataFrame(test_dense, columns=[f'tfid_{i}' for i in range(test_dense.shape[1])])\n",
    "    \n",
    "    # Add essay IDs for merging\n",
    "    test_tfidf_df['essay_id'] = test_df['essay_id'].values\n",
    "    \n",
    "    # Merge with original data\n",
    "    test_df = test_df.merge(test_tfidf_df, on='essay_id', how='left')\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "# Run the testing pipeline\n",
    "sub_df = pd.read_csv(f\"{BASE_PATH}/test.csv\")[[\"essay_id\"]].copy()\n",
    "sub_df[\"score\"] = xgb_testing_pipeline()\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Display the first 2 rows of the submission DataFrame\n",
    "print(sub_df.head())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 3596984,
     "sourceId": 6258399,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5043693,
     "sourceId": 8461056,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5048264,
     "sourceId": 8467210,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5050290,
     "sourceId": 8469926,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5050456,
     "sourceId": 8470138,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 184.056998,
   "end_time": "2024-05-20T18:45:18.064621",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-20T18:42:14.007623",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
